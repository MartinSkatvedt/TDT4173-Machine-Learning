{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras import initializers\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "import import_ipynb\n",
    "from DataProcessing import Data\n",
    "from xgboost_impl import Xgboost\n",
    "\n",
    "\n",
    "#physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('dataset/A/train_targets.parquet')\n",
    "train_b = pd.read_parquet('dataset/B/train_targets.parquet')\n",
    "train_c = pd.read_parquet('dataset/C/train_targets.parquet')\n",
    "\n",
    "X_train_observed_a = pd.read_parquet('dataset/A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('dataset/B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('dataset/C/X_train_observed.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('dataset/A/X_train_estimated.parquet') \n",
    "X_train_estimated_b = pd.read_parquet('dataset/B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('dataset/C/X_train_estimated.parquet')\n",
    "\n",
    "X_test_estimated_a = pd.read_parquet('dataset/A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('dataset/B/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('dataset/C/X_test_estimated.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(train_a, train_b, train_c, X_train_observed_a, X_train_observed_b, X_train_observed_c,\n",
    "                      X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_test_estimated_a, \n",
    "                      X_test_estimated_b, X_test_estimated_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = CatBoostRegressor(iterations=1000,\n",
    "                           depth=12,\n",
    "                           task_type=\"GPU\",\n",
    "                           devices='0:1',\n",
    "                           eval_metric=\"MAE\",\n",
    "                           random_seed=42)\n",
    "\n",
    "model_a.fit(data.A.train_x, data.A.train_y,\n",
    "            eval_set=(data.A.val_x, data.A.val_y),\n",
    "            early_stopping_rounds=50,\n",
    "            use_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space={\n",
    "        'depth': hp.quniform(\"depth\", 3, 12, 1),\n",
    "        'learning_rate': hp.uniform ('learning_rate', 1e-3, 0.1),\n",
    "    }\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = fmin(fn = objective,\n",
    "                        space = space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = 10,\n",
    "                        trials = trials)\n",
    "\n",
    "print(\"The best hyperparameters are : \",\"\\n\")\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "        \n",
    "    model = CatBoostRegressor(iterations=1000,\n",
    "                           depth=space['depth'],\n",
    "                           learning_rate=space['learning_rate'],\n",
    "                           task_type=\"GPU\",\n",
    "                           devices='0:1',\n",
    "                           eval_metric=\"MAE\",\n",
    "                           random_seed=42,\n",
    "                           silent=True)\n",
    "\n",
    "    building = data.C\n",
    "    \n",
    "    model.fit(building.train_x, building.train_y,\n",
    "                eval_set=(building.val_x, building.val_y),\n",
    "                early_stopping_rounds=50,\n",
    "                use_best_model=True)\n",
    "        \n",
    "        \n",
    "    val_pred = model.predict(building.val_x)\n",
    "    \n",
    "    mae_val = mean_absolute_error(val_pred, building.val_y)\n",
    "                    \n",
    "    return {'loss' : mae_val, 'status' : STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = Xgboost(data)\n",
    "\n",
    "xg.model_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, xs, ys, val_xs, val_ys, useTrainCV=True, cv_folds=5, early_stopping_rounds=10):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(xs, label=ys)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds, \n",
    "                          early_stopping_rounds=early_stopping_rounds, verbose_eval =True)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(xs, ys, eval_set=[(xs, ys),(val_xs, val_ys)], verbose=True)\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(xs)\n",
    "    dval_predictions = alg.predict(val_xs)\n",
    "    \n",
    "    mae = mean_absolute_error(dtrain_predictions, ys)\n",
    "    mae_val = mean_absolute_error(dval_predictions, val_ys)\n",
    "    print(mae, mae_val)\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = xgb.XGBRegressor(\n",
    " device='cuda',\n",
    " learning_rate =0.05,\n",
    " n_estimators=1693,\n",
    " max_depth=16,\n",
    " min_child_weight=9,\n",
    " gamma=4.709926652039647,\n",
    " subsample=0.5746022561573897,\n",
    " colsample_bytree=0.925119931399705,\n",
    " seed=42,\n",
    " eval_metric= 'mae',\n",
    " booster='gbtree',\n",
    " reg_alpha=77.7952642777926,\n",
    " reg_lambda=102.6220459955603,\n",
    ")\n",
    "\n",
    "modelfit(model_a, data.A.train_x, data.A.train_y, data.A.val_x, data.A.val_y, useTrainCV = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b = xgb.XGBRegressor(\n",
    " device='cuda',\n",
    " learning_rate =0.05,\n",
    " n_estimators=2880,\n",
    " max_depth=10,\n",
    " min_child_weight=2,\n",
    " gamma=6.9462927163070525,\n",
    " subsample=0.5425452253269976,\n",
    " colsample_bytree=0.8615770908405836,\n",
    " seed=42,\n",
    " eval_metric= 'mae',\n",
    " booster='gbtree',\n",
    " reg_alpha = 39.56391755892025,\n",
    " reg_lambda = 165.13746485969003,\n",
    ")\n",
    "\n",
    "modelfit(model_b, data.B.train_x, data.B.train_y, data.B.val_x, data.B.val_y, useTrainCV = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c = xgb.XGBRegressor(\n",
    " device='cuda',\n",
    " learning_rate =0.05,\n",
    " n_estimators=2049,\n",
    " max_depth=11,\n",
    " min_child_weight=6,\n",
    " gamma=2.700424640722136,\n",
    " subsample=0.7625820679319437,\n",
    " colsample_bytree=0.6696305568496206,\n",
    " seed=42,\n",
    " eval_metric= 'mae',\n",
    " booster='gbtree',\n",
    " reg_alpha = 146.95411105137276,\n",
    " reg_lambda = 125.49465203052867,\n",
    ")\n",
    "\n",
    "modelfit(model_c, data.C.train_x, data.C.train_y, data.C.val_x, data.C.val_y, useTrainCV = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_a_train = np.absolute(model_a.predict(train_data_a))\n",
    "preds_a_val = np.absolute(model_a.predict(val_data_a))\n",
    "preds_a_test = np.absolute(model_a.predict(test_a))\n",
    "\n",
    "preds_b_train = np.absolute(model_b.predict(train_data_b))\n",
    "preds_b_val = np.absolute(model_b.predict(val_data_b))\n",
    "preds_b_test = np.absolute(model_b.predict(test_b))\n",
    "\n",
    "preds_c_train = np.absolute(model_c.predict(train_data_c))\n",
    "preds_c_val = np.absolute(model_c.predict(val_data_c))\n",
    "preds_c_test = np.absolute(model_c.predict(test_c))\n",
    "\n",
    "fig, axs = plt.subplots(7, figsize=(10, 15))\n",
    "axs[0].plot((train_a_y), color=\"blue\")\n",
    "axs[0].plot(preds_a_train, color=\"red\", alpha=0.5)\n",
    "\n",
    "axs[1].plot((val_a_y), color=\"blue\")\n",
    "axs[1].plot(preds_a_val, color=\"red\", alpha=0.5)\n",
    "\n",
    "\n",
    "axs[2].plot((train_b_y), color=\"blue\")\n",
    "axs[2].plot(preds_b_train, color=\"red\", alpha=0.5)\n",
    "\n",
    "axs[3].plot((val_b_y), color=\"blue\")\n",
    "axs[3].plot(preds_b_val, color=\"red\", alpha=0.5)\n",
    "\n",
    "axs[4].plot((train_c_y), color=\"blue\")\n",
    "axs[4].plot(preds_c_train, color=\"red\", alpha=0.5)\n",
    "\n",
    "axs[5].plot((val_c_y), color=\"blue\")\n",
    "axs[5].plot(preds_c_val, color=\"red\", alpha=0.5)\n",
    "\n",
    "axs[6].plot(preds_a_test, color=\"blue\")\n",
    "axs[6].plot(preds_b_test, color=\"green\")\n",
    "axs[6].plot(preds_c_test, color=\"red\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "    \n",
    "    alg = xgb.XGBRegressor(\n",
    "     device='cuda',\n",
    "     learning_rate =0.2,\n",
    "     n_estimators= 500,\n",
    "     max_depth= round(space['max_depth']),\n",
    "     min_child_weight= space['min_child_weight'],\n",
    "     gamma=space['gamma'],\n",
    "     subsample= space['subsample'],\n",
    "     colsample_bytree= space['colsample_bytree'],\n",
    "     nthread=4,\n",
    "     seed=42,\n",
    "     eval_metric= 'mae',\n",
    "     booster='gbtree',\n",
    "     reg_lambda=space['reg_lambda'],\n",
    "     reg_alpha=space['reg_alpha'],\n",
    "     early_stopping_rounds = 10\n",
    "    )\n",
    "        #data.B.train_x, data.B.train_y, data.B.val_x, data.B.val_y\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(data.B.train_x, data.B.train_y, eval_set=[(data.B.train_x, data.B.train_y),\n",
    "                                                      (data.B.val_x, data.B.val_y)], verbose=False)\n",
    "        \n",
    "    #Predict training set:\n",
    "    dval_predictions = alg.predict( data.B.val_x)\n",
    "    \n",
    "    mae_val = mean_absolute_error(dval_predictions, data.B.val_y)\n",
    "                    \n",
    "    return {'loss' : mae_val, 'status' : STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space={\n",
    "        'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
    "        'gamma': hp.uniform ('gamma', 0,9),\n",
    "        'reg_alpha' : hp.uniform('reg_alpha', 0,180),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0,180),\n",
    "        'subsample' : hp.uniform('subsample', 0.5,1),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "    }\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = fmin(fn = objective,\n",
    "                        space = space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = 1000,\n",
    "                        trials = trials)\n",
    "\n",
    "print(\"The best hyperparameters are : \",\"\\n\")\n",
    "print(best_hyperparams)\n",
    "\n",
    "#{'colsample_bytree': 0.8563576286836063, 'gamma': 2.3530939377340916, 'max_depth': 17.0, 'min_child_weight': 9.0, 'reg_alpha': 5.362099435153887, 'reg_lambda': 64.39374187968356, 'subsample': 0.6745835850987192}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 30\n",
    "\n",
    "def compile_and_fit(model, train_x, train_y, val_x, val_y, patience=3):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=3,\n",
    "                                                    mode='min',\n",
    "                                                    restore_best_weights=True)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    history = model.fit(x= train_x, y=train_y, epochs=MAX_EPOCHS,\n",
    "                      validation_data=(val_x, val_y),\n",
    "                       callbacks=[early_stopping])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_batch(df):\n",
    "    df_labels = df.pop(\"pv_measurement\")\n",
    "    \n",
    "    BATCH_SIZE=30\n",
    "\n",
    "    df_arr = np.array(df, dtype=np.float32)\n",
    "    df_label_arr = np.array(df_labels, dtype=np.float32)\n",
    "\n",
    "    # Calculate the number of batches needed\n",
    "    num_batches = df_arr.shape[0] // BATCH_SIZE\n",
    "\n",
    "    # Reshape the 2D array into a 3D array with shape (num_batches, BATCH_SIZE, 48)\n",
    "    if df_arr.shape[0] % BATCH_SIZE == 0:\n",
    "        # If the data size is a multiple of BATCH_SIZE\n",
    "        data_3d = df_arr.reshape(num_batches, BATCH_SIZE, -1)\n",
    "        label_3d = df_label_arr.reshape(num_batches, BATCH_SIZE, -1)\n",
    "\n",
    "    else:\n",
    "        # If there's some remaining data that doesn't fit perfectly into batches\n",
    "        remaining_rows = df_arr.shape[0] % BATCH_SIZE\n",
    "        data_3d = df_arr[:-remaining_rows].reshape(num_batches, BATCH_SIZE, -1)\n",
    "        label_3d = df_label_arr[:-remaining_rows].reshape(num_batches, BATCH_SIZE, -1)\n",
    "\n",
    "    return data_3d, label_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(df):\n",
    "    BATCH_SIZE=30\n",
    "\n",
    "    df_arr = np.array(df, dtype=np.float32)\n",
    "\n",
    "    # Calculate the number of batches needed\n",
    "    num_batches = df_arr.shape[0] // BATCH_SIZE\n",
    "\n",
    "    # Reshape the 2D array into a 3D array with shape (num_batches, BATCH_SIZE, 48)\n",
    "    if df_arr.shape[0] % BATCH_SIZE == 0:\n",
    "        # If the data size is a multiple of BATCH_SIZE\n",
    "        data_3d = df_arr.reshape(num_batches, BATCH_SIZE, -1)\n",
    "\n",
    "    else:\n",
    "        # If there's some remaining data that doesn't fit perfectly into batches\n",
    "        remaining_rows = df_arr.shape[0] % BATCH_SIZE\n",
    "        data_3d = df_arr[:-remaining_rows].reshape(num_batches, BATCH_SIZE, -1)\n",
    "\n",
    "    return data_3d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a_x = batch(data.A.train_x)\n",
    "train_a_y = batch(data.A.train_y)\n",
    "val_a_x = batch(data.A.val_x)\n",
    "val_a_y = batch(data.A.val_y)\n",
    "\n",
    "train_b_x = batch(data.B.train_x)\n",
    "train_b_y = batch(data.B.train_y)\n",
    "val_b_x = batch(data.B.val_x)\n",
    "val_b_y = batch(data.B.val_y)\n",
    "\n",
    "train_c_x = batch(data.C.train_x)\n",
    "train_c_y = batch(data.C.train_y)\n",
    "val_c_x = batch(data.C.val_x)\n",
    "val_c_y = batch(data.C.val_y)\n",
    "\n",
    "test_a = batch(data.A.test_x)\n",
    "test_b = batch(data.B.test_x)\n",
    "test_c = batch(data.C.test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "lstm_model_a = tf.keras.models.Sequential([ \n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM( 32, return_sequences=True, kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                    bias_initializer=initializers.Constant(0.1))\n",
    "    ),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(48, return_sequences=True, dropout=0.5, bias_initializer=initializers.Constant(0.1))\n",
    "    ),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.5, bias_initializer=initializers.Constant(0.1))\n",
    "    ),\n",
    "\n",
    "    tf.keras.layers.Dense(units=1, activation=\"relu\", bias_initializer=initializers.Constant(0.1))\n",
    "\n",
    "])\n",
    "\n",
    "lstm_model_b = tf.keras.models.clone_model(lstm_model_a)\n",
    "lstm_model_c = tf.keras.models.clone_model(lstm_model_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "29/29 [==============================] - 39s 1s/step - loss: 2035815.5000 - mean_absolute_error: 771.3843 - val_loss: 1815520640.0000 - val_mean_absolute_error: 28730.8125\n",
      "Epoch 2/30\n",
      "12/29 [===========>..................] - ETA: 17s - loss: 1899136.3750 - mean_absolute_error: 747.8276"
     ]
    }
   ],
   "source": [
    "history_a = compile_and_fit(lstm_model_a,train_a_x, train_a_y, val_a_x, val_a_y)\n",
    "print(f'measure a: {lstm_model_a.evaluate(val_a_x, val_a_y)}')\n",
    "\n",
    "#[0.0038667283952236176, 0.024362364783883095]\n",
    "#[0.004080631770193577, 0.023939160630106926]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_b = compile_and_fit(lstm_model_b,train_b_x, train_b_y, val_b_x, val_b_y)\n",
    "print(f'measure b: {lstm_model_b.evaluate(val_b_x, val_b_y)}')\n",
    "\n",
    "# [0.003871380351483822, 0.023782264441251755]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_c = compile_and_fit(lstm_model_c,train_c_x, train_c_y, val_c_x, val_c_y)\n",
    "print(f'measure c: {lstm_model_c.evaluate(val_c_x, val_c_y)}')\n",
    "\n",
    "#[0.0031701738480478525, 0.023126540705561638]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm_model_a.get_weight_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_a = lstm_model_a.predict(test_a)\n",
    "pred_b = lstm_model_b.predict(test_b)\n",
    "pred_c = lstm_model_c.predict(test_c)\n",
    "\n",
    "pred_a_val = lstm_model_a.predict(val_a_x)\n",
    "pred_b_val = lstm_model_b.predict(val_b_x)\n",
    "pred_c_val = lstm_model_c.predict(val_c_x)\n",
    "\n",
    "pred_a_train = lstm_model_a.predict(train_a_x)\n",
    "pred_b_train = lstm_model_b.predict(train_b_x)\n",
    "pred_c_train = lstm_model_c.predict(train_c_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_a = pred_a.flatten() \n",
    "pred_b = pred_b.flatten() \n",
    "pred_c = pred_c.flatten() \n",
    "\n",
    "pred_a_val = pred_a_val.flatten() \n",
    "pred_b_val = pred_b_val.flatten() \n",
    "pred_c_val = pred_c_val.flatten() \n",
    "\n",
    "pred_a_train = pred_a_train.flatten() \n",
    "pred_b_train = pred_b_train.flatten() \n",
    "pred_c_train = pred_c_train.flatten() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm_model_a.save(\"./a_2.keras\")\n",
    "#lstm_model_b.save(\"./b_2.keras\")\n",
    "#lstm_model_c.save(\"./c_2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(pred_a, color=\"blue\")\n",
    "plt.plot(pred_b, color=\"red\")\n",
    "plt.plot(pred_c, color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(3)\n",
    "\n",
    "axs[0].plot(train_a_y.flatten(), color=\"blue\")\n",
    "axs[0].plot(pred_a_train, color=\"orange\")\n",
    "\n",
    "axs[1].plot(train_b_y.flatten(), color=\"blue\")\n",
    "axs[1].plot(pred_b_train, color=\"orange\")\n",
    "\n",
    "axs[2].plot(train_c_y.flatten(), color=\"blue\")\n",
    "axs[2].plot(pred_c_train, color=\"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3)\n",
    "\n",
    "axs[0].plot(val_a_y.flatten(), color=\"blue\")\n",
    "axs[0].plot(pred_a_val, color=\"orange\")\n",
    "\n",
    "axs[1].plot(val_b_y.flatten(), color=\"blue\")\n",
    "axs[1].plot(pred_b_val, color=\"orange\")\n",
    "\n",
    "axs[2].plot(val_c_y.flatten(), color=\"blue\")\n",
    "axs[2].plot(pred_c_val, color=\"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = np.concatenate((np.concatenate((preds_a_test, preds_b_test)),preds_c_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "\n",
    "# Example, let the predictions be random values\n",
    "test['prediction'] = submit\n",
    "sample_submission = sample_submission[['id']].merge(test[['id', 'prediction']], on='id', how='left')\n",
    "sample_submission.to_csv(\"xgboost_4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
